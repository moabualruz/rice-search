# Rice Search Platform - Docker Compose
# Cross-platform: Windows+WSL2, Linux, macOS (CPU)
# All data persisted to ./data via bind mounts

services:
  # ============================================
  # Infrastructure: etcd (Milvus metadata store)
  # ============================================
  etcd:
    container_name: rice-etcd
    image: quay.io/coreos/etcd:v3.5.17
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ./data/etcd:/etcd
    command: >
      etcd
      --data-dir=/etcd
      --listen-client-urls=http://0.0.0.0:2379
      --advertise-client-urls=http://etcd:2379
      --listen-peer-urls=http://0.0.0.0:2380
      --initial-advertise-peer-urls=http://etcd:2380
      --initial-cluster=default=http://etcd:2380
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped
    networks:
      - rice-network

  # ============================================
  # Infrastructure: Redis (Job queue for BullMQ)
  # ============================================
  redis:
    container_name: rice-redis
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - ./data/redis:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - rice-network

  # ============================================
  # Infrastructure: MinIO (Milvus object storage)
  # ============================================
  minio:
    container_name: rice-minio
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - ./data/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped
    networks:
      - rice-network

  # ============================================
  # Milvus: Vector database (standalone mode)
  # ============================================
  milvus:
    container_name: rice-milvus
    image: milvusdb/milvus:v2.4.15
    command: ["milvus", "run", "standalone"]
    security_opt:
      - seccomp:unconfined
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      MINIO_ACCESS_KEY_ID: minioadmin
      MINIO_SECRET_ACCESS_KEY: minioadmin
      MINIO_USE_SSL: "false"
      MINIO_REGION: us-east-1
      
      # ========== PROXY (Connection Layer) ==========
      PROXY_MAX_CONNECTIONS: "65535"           # Max client connections
      PROXY_MAX_TASK_NUM: "2048"               # Max queued tasks (doubled)
      PROXY_MAXMSGSIZE: "268435456"            # 256MB max message size
      PROXY_GINLOGGING: "false"                # Disable verbose logging
      
      # ========== QUERY NODE (Search Performance) ==========
      QUERYNODE_SCHEDULER_MAXREADCONCURRENCY: "256"     # Parallel read ops
      QUERYNODE_SCHEDULER_SCHEDULETIMEOUT: "10000"      # 10s schedule timeout
      QUERYNODE_GROUPING_ENABLED: "true"                # Group similar queries
      QUERYNODE_GROUPING_MAXNQ: "1000"                  # Max queries per group
      QUERYNODE_SEGCORE_CHUNKROWS: "1024"               # Rows per chunk
      QUERYNODE_CACHE_ENABLED: "true"
      QUERYNODE_CACHE_SIZE: "2147483648"                # 2GB query cache
      QUERYNODE_LOADMEMORYLIMIT: "6442450944"           # 6GB memory limit
      
      # ========== DATA NODE (Indexing Performance) ==========
      DATANODE_FLUSH_INSERT_BUFFER_SIZE: "268435456"    # 256MB insert buffer
      DATANODE_FLOWGRAPH_MAXQUEUELEN: "16"              # Queue length
      DATANODE_FLOWGRAPH_MAXPARALLELISM: "8"            # Parallel flush ops
      DATANODE_SEGMENT_INSERTBUFFERSIZE: "16777216"     # 16MB per segment buffer
      
      # ========== INDEX NODE (Index Building) ==========
      INDEXNODE_SCHEDULER_BUILDPARALLEL: "4"            # Parallel index builds
      INDEXNODE_ENABLEDISK: "true"                      # Enable disk index
      
      # ========== COORD (Coordination) ==========
      QUERYCOORD_TASK_MERGEIDX: "16"                    # Merge task parallelism
      QUERYCOORD_SCHEDULERTASKQUEUECAP: "10000"         # Task queue capacity
      DATACOORD_SEGMENT_MAXSIZE: "1024"                 # 1GB max segment
      DATACOORD_SEGMENT_SEALEDMAXSIZE: "2048"           # 2GB sealed segment
      DATACOORD_COMPACTION_ENABLED: "true"              # Auto compaction
      
      # ========== gRPC PERFORMANCE ==========
      GRPC_SERVERMAXTOKENPERSET: "1073741824"           # 1GB max tokens
      COMMON_MAXMSGSIZE: "268435456"                    # 256MB max message
      GRPC_CLIENTMAXSENDSIZE: "268435456"               # 256MB client send
      GRPC_CLIENTMAXRECVSIZE: "268435456"               # 256MB client recv
      GRPC_SERVERMAXSENDSIZE: "268435456"               # 256MB server send
      GRPC_SERVERMAXRECVSIZE: "268435456"               # 256MB server recv
      
      # ========== MEMORY & STORAGE ==========
      COMMON_STORAGETYPE: "local"                       # Local storage mode
      QUOTAANDLIMITS_ENABLED: "true"                    # Enable resource quotas
      QUOTAANDLIMITS_DDLENABLED: "false"                # No DDL rate limit
      QUOTAANDLIMITS_INDEXRATEENABLED: "false"          # No index rate limit
      QUOTAANDLIMITS_FLUSHRATEENABLED: "false"          # No flush rate limit
      
      # ========== GENERAL ==========
      COMMON_GRACEFUL_TIME: "5000"                      # Faster shutdown
      COMMON_GRACEFUL_STOP_TIMEOUT: "30"
      LOG_LEVEL: "warn"                                 # Reduce log noise
    volumes:
      - ./data/milvus:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 5
    depends_on:
      etcd:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    networks:
      - rice-network

  # ============================================
  # Infinity: Embedding + Reranking Server (CPU)
  # 
  # Default Models (Dec 2025 - optimized for code search):
  # - jinaai/jina-code-embeddings-1.5b (1536d, Rust/Kotlin/15+ languages)
  # - jinaai/jina-reranker-v2-base-multilingual (code-optimized, 60-80ms)
  #
  # Alternative models (set via .env or environment):
  # Embeddings:
  #   - jinaai/jina-code-embeddings-0.5b (smaller, 1536d)
  #   - nomic-ai/nomic-embed-code (896d, no Rust/Kotlin)
  #   - mixedbread-ai/mxbai-embed-large-v1 (1024d, general purpose)
  # Rerankers:
  #   - BAAI/bge-reranker-v2-m3 (multilingual)
  #   - mixedbread-ai/mxbai-rerank-xsmall-v1 (general purpose)
  #
  # For GPU, use docker-compose.gpu.yml override
  # ============================================
  infinity:
    container_name: rice-infinity
    image: michaelf34/infinity:latest-cpu
    command: >
      v2
      --model-id ${EMBED_MODEL:-jinaai/jina-code-embeddings-1.5b}
      --model-id ${RERANK_MODEL:-jinaai/jina-reranker-v2-base-multilingual}
      --engine ${INFINITY_ENGINE:-torch}
      --port 80
      --batch-size ${INFINITY_BATCH_SIZE:-32}
    volumes:
      - ./data/infinity-cache:/app/.cache
    ports:
      - "8081:80"
    environment:
      - HF_HOME=/app/.cache
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      start_period: 180s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - rice-network

  # ============================================
  # API: Main integration point
  # All clients connect here (8080)
  # ============================================
  api:
    container_name: rice-api
    build:
      context: ./api
      dockerfile: Dockerfile
    environment:
      # Service connections
      MILVUS_HOST: milvus
      MILVUS_PORT: "19530"
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      
      # Infinity (embedding + reranking)
      # Models are configured in infinity service, these are for API reference
      INFINITY_URL: http://infinity:80
      INFINITY_EMBED_MODEL: ${EMBED_MODEL:-jinaai/jina-code-embeddings-1.5b}
      INFINITY_RERANK_MODEL: ${RERANK_MODEL:-jinaai/jina-reranker-v2-base-multilingual}
      
      # Data directories
      DATA_DIR: /data
      TANTIVY_INDEX_DIR: /tantivy
      
      # Model configuration
      # Jina Code: 1536d | Nomic Code: 896d | mxbai: 1024d
      EMBEDDING_DIM: ${EMBEDDING_DIM:-1536}
      
      # Search configuration
      SPARSE_TOPK: "100"
      DENSE_TOPK: "100"
      FINAL_TOPK: "20"
      RRF_K: "60"
      
      # Reranking configuration
      # Jina Reranker v2: 60-80ms on CPU (code-optimized)
      RERANK_ENABLED: "true"
      RERANK_CANDIDATES: "30"
      RERANK_TIMEOUT_MS: "100"
      
      # Auth (disabled by default)
      AUTH_MODE: none
      
      # Logging
      LOG_LEVEL: INFO
      
      # Single process mode - Bun async I/O handles concurrency well
      # Clustering disabled to simplify Tantivy queue serialization
      CLUSTER_MODE: "false"
      NODE_ENV: production
      
      # Bun runtime tuning
      BUN_CONFIG_MAX_HTTP_CONNECTIONS: "1024"
    volumes:
      - ./data/api:/data
      - ./data/tantivy:/tantivy
      # Mount repos for server-side indexing (optional)
      - ${REPOS_PATH:-./repos}:/repos:ro
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 15s
      timeout: 10s
      retries: 3
    depends_on:
      milvus:
        condition: service_healthy
      infinity:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    networks:
      - rice-network

  # ============================================
  # Web UI: Simple search interface
  # Commented out for initial testing - can access API directly
  # ============================================
  web-ui:
    container_name: rice-web-ui
    build:
      context: ./web-ui
      dockerfile: Dockerfile
    environment:
      # For server-side proxying within Docker network
      API_URL: http://api:8080
    ports:
      - "3000:3000"
    depends_on:
      api:
          condition: service_healthy
    restart: unless-stopped
    networks:
      - rice-network

networks:
  rice-network:
    driver: bridge
    name: rice-search
