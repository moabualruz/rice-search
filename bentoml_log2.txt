
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
(EngineCore_DP0 pid=95) /usr/local/lib/python3.10/dist-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
==========
(EngineCore_DP0 pid=95) We recommend installing via `pip install torch-c-dlpack-ext`
(EngineCore_DP0 pid=95)   warnings.warn(
(EngineCore_DP0 pid=95) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=95) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:45<02:16, 45.51s/it]
(EngineCore_DP0 pid=95) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [01:32<01:33, 46.55s/it]
(EngineCore_DP0 pid=95) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [02:15<00:44, 44.65s/it]
== CUDA ==
==========

CUDA Version 12.1.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
(EngineCore_DP0 pid=95) 

Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:26<00:00, 31.33s/it]
*************************
** DEPRECATION NOTICE! **
(EngineCore_DP0 pid=95) 
*************************
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:26<00:00, 36.52s/it]
THIS IMAGE IS DEPRECATED and is scheduled for DELETION.
(EngineCore_DP0 pid=95) 
    https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/support-policy.md

2026-01-03T15:17:25+0000 [INFO] [cli] Starting production HTTP BentoServer from "service:RiceInferenceService" listening on http://localhost:3001 (Press CTRL+C to quit)
INFO 01-03 15:17:58 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 8192, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen2.5-Coder-7B-Instruct'}
INFO 01-03 15:18:08 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-03 15:18:08 [model.py:1661] Using max model len 8192
INFO 01-03 15:18:08 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 01-03 15:18:08 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
INFO 01-03 15:18:08 [vllm.py:722] Cudagraph is disabled under eager mode
WARNING 01-03 15:18:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
WARNING 01-03 15:18:10 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:18:16 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='Qwen/Qwen2.5-Coder-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=Qwen/Qwen2.5-Coder-7B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:18:16 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.18.0.6:50751 backend=nccl
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:18:16 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=95)[0;0m WARNING 01-03 15:18:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:18:17 [gpu_model_runner.py:3562] Starting to load model Qwen/Qwen2.5-Coder-7B-Instruct...
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:18:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:20:51 [default_loader.py:308] Loading weights took 146.12 seconds
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:20:51 [gpu_model_runner.py:3659] Model loading took 14.2488 GiB memory and 152.751635 seconds
[0;36m(EngineCore_DP0 pid=95)[0;0m INFO 01-03 15:22:37 [gpu_worker.py:375] Available KV cache memory: -7.98 GiB
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 637, in __init__
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]     super().__init__(
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=95) Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]     check_enough_kv_cache_memory(
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866]     raise ValueError(
[0;36m(EngineCore_DP0 pid=95)[0;0m ERROR 01-03 15:22:37 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
(EngineCore_DP0 pid=95) Traceback (most recent call last):
(EngineCore_DP0 pid=95)   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=95)     self.run()
(EngineCore_DP0 pid=95)   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=95)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=95)   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=95)     raise e
(EngineCore_DP0 pid=95)   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 857, in run_engine_core
2026-01-03T15:22:39+0000 [WARNING] [entry_service:rice-inference:1] Failed to load LLM: Engine core initialization failed. See root cause above. Failed core proc(s): {}. Chat endpoint will be disabled.
2026-01-03T15:22:39+0000 [INFO] [entry_service:rice-inference:1] Service rice-inference initialized
(EngineCore_DP0 pid=95)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=95)   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=95)     super().__init__(
(EngineCore_DP0 pid=95)   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=95)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=95)   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=95)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=95)   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=95)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=95)   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=95)     raise ValueError(
(EngineCore_DP0 pid=95) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W103 15:22:37.127193869 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
