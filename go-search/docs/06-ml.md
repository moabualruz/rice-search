# ML Inference

## Overview

All ML inference runs natively in Go via ONNX Runtime. No Python sidecars.

---

## Models

### Model Registry

| Model | Type | Purpose | Size | Dimensions | Tokens | GPU Default |
|-------|------|---------|------|------------|--------|-------------|
| Jina Code Embeddings 1.5B | embed | Dense embedding | ~1.5GB | 1536 | 8192 | ✅ |
| SPLADE v3 | sparse | Sparse encoding | ~250MB | Variable | 512 | ✅ |
| Jina Reranker v2 | rerank | Cross-encoder | ~800MB | 1 (score) | 512 | ✅ |
| **CodeBERT Base** | query | Query understanding | ~438MB | 768 | 512 | ✅ |

### Query Understanding Model: CodeBERT

**Default model**: `microsoft/codebert-base`

CodeBERT is a code-specialized transformer that converts natural language queries into optimized search terms:

| Feature | Description |
|---------|-------------|
| **Languages** | Pre-trained on 6 languages: Python, Java, JavaScript, PHP, Ruby, Go |
| **Intent Classification** | Detects: navigational, factual, exploratory, analytical |
| **Keyword Expansion** | Expands queries with code-aware synonyms and related terms |
| **Parameters** | 125M parameters (~438MB ONNX) |
| **Max Tokens** | 512 |

**Why CodeBERT over general models?**

| Aspect | General Models | CodeBERT |
|--------|----------------|----------|
| Code terminology | ⚠️ Limited | ✅ Native understanding |
| Function/class names | ⚠️ Treated as text | ✅ Semantic awareness |
| API patterns | ⚠️ No context | ✅ Trained on code |
| Query → Keywords | Basic extraction | Intelligent expansion |

**Alternative models available:**

| Model | Size | Use Case |
|-------|------|----------|
| `microsoft/codebert-base` | 438MB | **Default** - Best for code queries |
| `Salesforce/codet5p-220m` | 220MB | Alternative - Encoder-decoder |
| `kousik-2310/intent-classifier-minilm` | 20MB | Fast intent-only classification |

### Fallback Behavior

Query understanding has graceful fallback:

```
┌─────────────────┐
│ User Query      │
└────────┬────────┘
         │
         ▼
┌─────────────────┐     ┌─────────────────┐
│ CodeBERT Model  │────>│ Model Output    │
│ (if enabled)    │     │ - Intent        │
└────────┬────────┘     │ - Keywords      │
         │ failure      │ - Expanded      │
         ▼              │ - Confidence    │
┌─────────────────┐     └─────────────────┘
│ Keyword         │            │
│ Extraction      │────────────┘
│ (heuristic)     │
└─────────────────┘
```

When `QueryEnabled=false` or model fails:
- Uses `internal/query/keyword_extractor.go` for pattern-based extraction
- Still provides intent, keywords, code terms, and expansion
- ~10x faster but less semantic understanding

### Model Files

```
models/
├── splade-pp-en-v1.onnx           # Sparse encoder
├── jina-embeddings-v3.onnx        # Dense embedder  
├── jina-reranker-v2.onnx          # Reranker
└── tokenizer/
    ├── tokenizer.json             # Shared tokenizer
    └── vocab.txt                  # Vocabulary
```

### Model Download

```bash
# Download on first run
./rice-search models download

# Or specify custom URL
./rice-search models download --url https://custom.host/models/
```

---

## ONNX Runtime

### Go Bindings

Using `github.com/yalue/onnxruntime_go`:

```go
import ort "github.com/yalue/onnxruntime_go"

// Initialize runtime
ort.InitializeEnvironment()

// Load model
session, _ := ort.NewSession(
    "models/jina-embeddings-v3.onnx",
    []string{"input_ids", "attention_mask"},
    []string{"embeddings"},
)

// Run inference
outputs, _ := session.Run(inputs)
```

### Execution Providers

| Provider | Use When | Performance |
|----------|----------|-------------|
| CPU | No GPU available | 1x baseline |
| CUDA | NVIDIA GPU | 10-20x faster |
| TensorRT | NVIDIA + optimized | 15-30x faster |
| DirectML | Windows + any GPU | 5-10x faster |
| CoreML | macOS | 5-10x faster |

### Configuration

```bash
# CPU (default)
ONNX_PROVIDER=cpu

# NVIDIA GPU
ONNX_PROVIDER=cuda
CUDA_VISIBLE_DEVICES=0

# TensorRT (requires additional libs)
ONNX_PROVIDER=tensorrt
```

---

## GPU Model Loading

**Status**: Per-model GPU toggles are implemented. Load modes (all/ondemand/lru) are **planned for future**.

### Current Implementation

GPU acceleration is controlled **per model** via settings:

```bash
# Per-model GPU toggles (implemented)
RICE_EMBED_GPU=true          # GPU for embedding model
RICE_RERANK_GPU=true         # GPU for reranking model
RICE_QUERY_GPU=false         # GPU for query understanding (disabled by default)
```

All enabled models are loaded on startup and kept in memory.

### Future: Advanced Loading Modes

The following modes are **planned but not yet implemented**:

| Mode | Description | VRAM Required | Status |
|------|-------------|---------------|--------|
| `all` | All models loaded | ~2GB (FP16) | **Current behavior** |
| `ondemand` | Load when needed | ~600MB peak | Planned |
| `lru` | Keep N most recent | Configurable | Planned |

#### Planned Mode: `ondemand`

```
Request → Load model → Inference → Unload after timeout
```

Best for: Shared GPU, low memory.

#### Planned Mode: `lru`

```
Keep last N models loaded, evict least recently used.
```

Best for: Medium memory, mixed workloads.

---

## Inference Details

### Dense Embedding (Jina)

```go
// Input
text := "func Authenticate(ctx context.Context) error"

// Tokenize
tokens := tokenizer.Encode(text)  // [CLS] func Auth... [SEP]
inputIDs := tokens.IDs            // []int64
attentionMask := tokens.Mask      // []int64

// Inference
outputs := embedSession.Run(inputIDs, attentionMask)

// Output: [1, 1536] tensor
embedding := outputs[0].Float32s()

// Normalize (L2)
embedding = normalize(embedding)
```

### Sparse Encoding (SPLADE)

```go
// Input
text := "authentication handler"

// Tokenize
tokens := tokenizer.Encode(text)

// Inference (SPLADE outputs logits per token)
outputs := spladeSession.Run(tokens.IDs, tokens.Mask)

// Output: [1, seq_len, vocab_size] tensor
// Apply ReLU + log(1 + x) + max pooling
logits := outputs[0].Float32s()
sparse := spladePooling(logits)

// Convert to sparse format
indices, values := toSparse(sparse, threshold=0.01)
```

### Reranking (Jina)

```go
// Input
query := "authentication"
documents := []string{"func auth()...", "func login()..."}

// Create pairs
pairs := make([]string, len(documents))
for i, doc := range documents {
    pairs[i] = query + " [SEP] " + doc
}

// Batch tokenize
tokens := tokenizer.EncodeBatch(pairs)

// Inference
outputs := rerankSession.Run(tokens)

// Output: [batch, 1] scores
scores := outputs[0].Float32s()

// Sort by score
ranked := sortByScore(documents, scores)
```

---

## Caching

### Embedding Cache

Same text = same embedding. Cache forever.

```go
type EmbedCache struct {
    cache map[string][]float32  // hash(text) → embedding
    mu    sync.RWMutex
}

func (c *EmbedCache) Get(text string) ([]float32, bool) {
    key := sha256(text)
    c.mu.RLock()
    defer c.mu.RUnlock()
    emb, ok := c.cache[key]
    return emb, ok
}
```

### Cache Storage

| Mode | Storage | Persistence |
|------|---------|-------------|
| Single process | In-memory map | No |
| Distributed | Redis | Yes |

### Cache Size

```bash
# Max cache entries (default: 100,000)
EMBED_CACHE_SIZE=100000

# Estimated memory: 100K * 1536 * 4 bytes = ~600MB
```

---

## Batching

### Why Batch?

| Batch Size | Throughput | Latency |
|------------|------------|---------|
| 1 | 10 texts/sec | 100ms |
| 8 | 60 texts/sec | 133ms |
| 32 | 150 texts/sec | 213ms |

### Configuration

```bash
# Embedding batch size
EMBED_BATCH_SIZE=32

# Sparse batch size (future - not yet configurable)
SPARSE_BATCH_SIZE=32

# Rerank batch size (all at once for single query)
RERANK_BATCH_SIZE=64
```

**Note**: Batch timeout configuration is **not yet implemented**. Models process batches immediately without waiting.

---

## Error Handling

### Model Load Errors

```go
if err := loadModel("embed"); err != nil {
    // Try fallback model
    if err := loadModel("embed-fallback"); err != nil {
        // Fatal: service cannot start
        log.Fatal("cannot load embedding model")
    }
}
```

### Inference Errors

```go
result, err := session.Run(inputs)
if err != nil {
    // Retry once
    result, err = session.Run(inputs)
    if err != nil {
        // Return error to caller
        return nil, fmt.Errorf("inference failed: %w", err)
    }
}
```

### OOM Handling

```go
// Monitor GPU memory
if gpuMemoryUsage() > 0.9 {
    // Evict least used model
    evictLRU()
}
```

---

## Performance Targets

| Operation | Target (GPU) | Target (CPU) |
|-----------|--------------|--------------|
| Single embed | <50ms | <200ms |
| Batch embed (32) | <200ms | <2s |
| Single sparse | <30ms | <100ms |
| Batch sparse (32) | <150ms | <1.5s |
| Rerank (30 docs) | <80ms | <500ms |

---

## Quantization

Models can be quantized for faster inference and lower memory.

| Format | Size | Speed | Quality |
|--------|------|-------|---------|
| FP32 | 100% | 1x | 100% |
| FP16 | 50% | 1.5x | ~100% |
| INT8 | 25% | 2-3x | 99%+ |

### Configuration

```bash
# Use FP16 models (default)
EMBED_MODEL=jina-embeddings-v3-fp16.onnx

# Use INT8 models (faster, slightly lower quality)
EMBED_MODEL=jina-embeddings-v3-int8.onnx
```

---

## Event Bus Integration

The ML service is fully integrated with the event bus for decoupled communication with Search and Index services.

### Event Handlers

The ML service registers three event handlers on startup (`internal/ml/handlers.go`):

| Topic | Handler | Response Topic | Purpose |
|-------|---------|----------------|---------|
| `ml.embed.request` | `handleEmbed()` | `ml.embed.response` | Generate dense embeddings |
| `ml.sparse.request` | `handleSparse()` | `ml.sparse.response` | Generate sparse (SPLADE) vectors |
| `ml.rerank.request` | `handleRerank()` | `ml.rerank.response` | Rerank documents |

### Request/Response Flow

```
Search/Index Service              Event Bus                    ML Service
        │                            │                             │
        │  ml.embed.request          │                             │
        │  { texts: ["..."] }        │                             │
        │───────────────────────────>│───────────────────────────>│
        │                            │                             │
        │                            │         handleEmbed()       │
        │                            │         → Embed(texts)      │
        │                            │                             │
        │  ml.embed.response         │                             │
        │  { embeddings: [[...]] }   │                             │
        │<───────────────────────────│<───────────────────────────│
```

### Event Handler Registration

```go
// From internal/ml/handlers.go
func (h *EventHandler) Register(ctx context.Context) error {
    handlers := map[string]bus.Handler{
        bus.TopicEmbedRequest:  h.handleEmbed,
        bus.TopicSparseRequest: h.handleSparse,
        bus.TopicRerankRequest: h.handleRerank,
    }
    for topic, handler := range handlers {
        if err := h.bus.Subscribe(ctx, topic, handler); err != nil {
            return err
        }
    }
    return nil
}
```

### Clients Using Event Bus

**Search Service** (`internal/search/service.go`):
- `embedViaEventBus()` - Requests dense embeddings
- `sparseEncodeViaEventBus()` - Requests sparse vectors
- `rerankViaEventBus()` - Requests document reranking

**Index Pipeline** (`internal/index/pipeline.go`):
- `embedViaEventBus()` - Requests dense embeddings for chunks
- `sparseEncodeViaEventBus()` - Requests sparse vectors for chunks

### Fallback Behavior

Both Search and Index services implement graceful fallback:

1. **Event bus available**: Uses `bus.Request()` for ML operations
2. **Event bus unavailable**: Falls back to direct `ml.Embed()`, `ml.SparseEncode()`, `ml.Rerank()` calls
3. **Event bus timeout**: Falls back to direct ML calls

This ensures the system remains functional even without the event bus, while providing loose coupling and observability benefits when available.

### Benefits of Event-Driven ML

| Benefit | Description |
|---------|-------------|
| **Loose Coupling** | Search/Index don't need direct ML service dependency |
| **Observability** | All ML operations are traceable via event bus |
| **Scalability** | ML service can be scaled independently |
| **Resilience** | Automatic fallback maintains functionality |
| **Metrics** | Centralized metrics collection via event subscriptions |
