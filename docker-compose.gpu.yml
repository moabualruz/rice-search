# GPU Override for Rice Search Platform
# Usage: docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
#
# Requirements:
#   - NVIDIA GPU with CUDA support
#   - nvidia-container-toolkit installed
#   - Docker configured with nvidia runtime
#
# GPU Memory Requirements (approximate):
#   - Infinity (embeddings + reranking): ~4-6GB VRAM
#   - BGE-M3 (if using): ~4-6GB VRAM
#   - Milvus: ~2-4GB VRAM depending on index size
#   - Total recommended: 8GB+ VRAM

services:
  # ============================================
  # Milvus with GPU acceleration
  # Accelerates vector similarity search
  # ============================================
  milvus:
    image: milvusdb/milvus:v2.4.15-gpu
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      MINIO_ACCESS_KEY_ID: minioadmin
      MINIO_SECRET_ACCESS_KEY: minioadmin
      MINIO_USE_SSL: "false"
      MINIO_REGION: us-east-1
      # GPU Configuration
      CUDA_VISIBLE_DEVICES: "0"
      # Enable GPU for indexing and search
      KNOWHERE_GPU_MEM_POOL_SIZE: "2048:4096"
      # Performance tuning (inherited + GPU specific)
      QUERY_NODE_MAX_CONCURRENT: "512"
      PROXY_MAX_TASK_NUM: "2048"
      QUERYNODE_CACHE_ENABLED: "true"
      QUERYNODE_CACHE_SIZE: "4294967296"  # 4GB cache with GPU
      # gRPC message size limits (fixes RESOURCE_EXHAUSTED on large batches)
      GRPC_SERVERMAXTOKENPERSET: "1073741824"  # 1GB max tokens
      PROXY_MAXMSGSIZE: "268435456"            # 256MB max message size
      COMMON_MAXMSGSIZE: "268435456"           # 256MB max message size
      # Insert buffer tuning
      DATANODE_FLUSH_INSERT_BUFFER_SIZE: "268435456"  # 256MB insert buffer
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ============================================
  # Infinity with GPU acceleration
  # Latest stable versions (Dec 2025):
  # - mixedbread-ai/mxbai-embed-large-v1 (1024d embeddings, SOTA quality)
  # - mixedbread-ai/mxbai-rerank-xsmall-v1 (officially supported reranker)
  # ~10x faster embedding generation with GPU
  # ============================================
  infinity:
    image: michaelf34/infinity:latest
    command: >
      v2
      --model-id mixedbread-ai/mxbai-embed-large-v1
      --model-id mixedbread-ai/mxbai-rerank-xsmall-v1
      --engine torch
      --port 80
      --batch-size 64
      --device cuda
    environment:
      - HF_HOME=/app/.cache
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - INFINITY_ENGINE=torch
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]


