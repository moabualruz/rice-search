# Model Registry for unified-inference
# Using Ollama for automatic memory management and GPU/CPU support

models:
  # ============================================================================
  # OLLAMA MODELS (GPU/CPU with automatic memory management)
  # ============================================================================

  # Embedding model - DEFAULT
  - name: nomic-embed
    type: embedding
    execution_mode: gpu  # Ollama auto-switches between GPU/CPU
    backend: ollama
    model_path: nomic-embed-text  # Ollama model name
    format: ollama
    gpu_id: 0
    port: 11434  # Ollama shared port
    idle_timeout: 0  # Never unload
    is_embedding: true
    trust_remote_code: false
    dtype: auto
    default: true  # Default embedding model

  # Reranking model - DEFAULT (using LLM with reranking prompt)
  - name: qwen-reranker
    type: rerank
    execution_mode: gpu
    backend: ollama
    model_path: qwen2.5-coder:1.5b  # Use LLM for reranking
    format: ollama
    gpu_id: 0
    port: 11434
    idle_timeout: 300
    is_embedding: false
    trust_remote_code: false
    dtype: auto
    default: true  # Default rerank model

  # LLM model - DEFAULT
  - name: qwen-coder-1.5b
    type: llm
    execution_mode: gpu
    backend: ollama
    model_path: qwen2.5-coder:1.5b  # Ollama model name
    format: ollama
    gpu_id: 0
    port: 11434  # Ollama shared port
    idle_timeout: 600
    is_embedding: false
    trust_remote_code: false
    dtype: auto
    default: true  # Default LLM model

# Notes:
# - DEFAULT models are auto-selected based on endpoint (no 'model' parameter needed)
# - Custom models require explicit 'model' parameter in requests
# - execution_mode is STATIC per model instance (requires restart to change)
# - GPU models use SGLang with --max-running-requests 3 --max-total-tokens 0
# - CPU models use separate CPU backend (llama.cpp, ggml, etc.)
# - GGUF is NOT supported by SGLang (GPU mode)
# - cpu_offload_model specifies which CPU model to route to when GPU is overloaded
