services:
  # ---------------------------------------------------------------------------
  # INFRASTRUCTURE
  # ---------------------------------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - ../data/qdrant:/qdrant/storage
    restart: always
    networks:
      - rice-net

  redis:
    image: redis:8-alpine
    ports:
      - "6379:6379" 
    volumes:
      - ../data/redis:/data
    restart: always
    networks:
      - rice-net

  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ../data/minio:/data
    restart: always
    networks:
      - rice-net

  # ---------------------------------------------------------------------------
  # UNIFIED INFERENCE - Python Router/Orchestrator for SGLang Backends
  # Manages multiple SGLang model instances with dynamic lifecycle
  # ---------------------------------------------------------------------------
  unified-inference:
    build:
      context: ./unified-inference
      dockerfile: Dockerfile
    ports:
      - "3001:3001"      # Router API
      - "30001-30020:30001-30020"  # SGLang backends (dynamic allocation)
    volumes:
      - ./unified-inference:/app  # Source code (hot reload)
      - ../data/hf_cache:/root/.cache/huggingface  # HuggingFace model cache
      - pip-cache-unified-inference:/root/.cache/pip  # Pip cache for unified-inference
    environment:
      # Execution mode (gpu or cpu)
      - EXECUTION_MODE=${EXECUTION_MODE:-gpu}

      # Router configuration
      - ROUTER_HOST=0.0.0.0
      - ROUTER_PORT=3001

      # SGLang backend configuration (ENFORCED)
      - SGLANG_MAX_RUNNING_REQUESTS=3
      - SGLANG_MAX_TOTAL_TOKENS=0  # Elastic memory

      # Model registry
      - MODELS_CONFIG_PATH=/app/config/models.yaml

      # Lifecycle management
      - DEFAULT_IDLE_TIMEOUT=${DEFAULT_IDLE_TIMEOUT:-300}
      - HEALTH_CHECK_INTERVAL=10
      - STARTUP_TIMEOUT=120

      # CPU offload (GPU mode only)
      - ENABLE_CPU_OFFLOAD=${ENABLE_CPU_OFFLOAD:-true}
      - OFFLOAD_QUEUE_THRESHOLD=3

      # HuggingFace
      - HF_HOME=/root/.cache/huggingface
      - HF_TOKEN=${HF_TOKEN}

      # GPU selection
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    networks:
      - rice-net

  # ---------------------------------------------------------------------------
  # TANTIVY - Rust BM25 Lexical Search Service
  # ---------------------------------------------------------------------------
  tantivy:
    build:
      context: ../rust-tantivy
      dockerfile: Dockerfile
    ports:
      - "3002:3002"
    volumes:
      - ../data/tantivy:/data  # Persistent BM25 index (file-based for easy reset)
    environment:
      - RUST_LOG=info
      - TANTIVY_DATA_DIR=/data
      - PORT=3002
      - HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - rice-net

  # ---------------------------------------------------------------------------
  # PYTHON BACKEND (API & WORKER)
  # ---------------------------------------------------------------------------

  backend-api:
    build:
      context: ../backend
      dockerfile: Dockerfile
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379/0
      - QDRANT_URL=http://qdrant:6333
      - MINIO_ENDPOINT=minio:9000
      - BENTOML_URL=http://unified-inference:3001
      - TANTIVY_URL=http://tantivy:3002
      - SHARED_TMP_DIR=/shared/ingest
      - RERANK_MODE=${RERANK_MODE:-rerank}
    volumes:
      - ../backend:/app
      - ../data/ingest-shared:/shared/ingest  # Shared ingestion temp files (file-based)
      - pip-cache-backend:/root/.cache/pip  # Separate pip cache for backend
    depends_on:
      - redis
      - qdrant
      - unified-inference
    networks:
      - rice-net

  backend-worker:
    build:
      context: ../backend
      dockerfile: Dockerfile
    command: python src/worker/start_worker.py
    environment:
      - REDIS_URL=redis://redis:6379/0
      - QDRANT_URL=http://qdrant:6333
      - MINIO_ENDPOINT=minio:9000
      - BENTOML_URL=http://unified-inference:3001
      - TANTIVY_URL=http://tantivy:3002
      - SHARED_TMP_DIR=/shared/ingest
      - RERANK_MODE=${RERANK_MODE:-rerank}
    volumes:
      - ../backend:/app
      - ../data/ingest-shared:/shared/ingest  # Shared ingestion temp files (file-based)
      - pip-cache-backend:/root/.cache/pip  # Shared with backend-api (same dependencies)
    depends_on:
      - redis
      - qdrant
      - unified-inference
    networks:
      - rice-net

  # ---------------------------------------------------------------------------
  # FRONTEND (NEXT.JS)
  # ---------------------------------------------------------------------------
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    command: npm run dev
    ports:
      - "3000:3000"
    volumes:
      - ../frontend:/app
      - /app/node_modules
      - npm-cache:/root/.npm  # Persistent npm cache
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    extra_hosts:
      - "localhost:host-gateway"
    networks:
      - rice-net

  # ---------------------------------------------------------------------------
  # TESTS (Optional)
  # ---------------------------------------------------------------------------
  tests-e2e:
    profiles: ["test"]
    build:
      context: ../backend
      dockerfile: Dockerfile.e2e
    working_dir: /app
    volumes:
      - ../backend:/app
    environment:
      - FRONTEND_URL=http://frontend:3000
      - REDIS_URL=redis://redis:6379/0
      - QDRANT_URL=http://qdrant:6333
    command: pytest --no-cov tests/e2e
    depends_on:
      - frontend
      - backend-api
    networks:
      - rice-net

networks:
  rice-net:
    driver: bridge

volumes:
  pip-cache-unified-inference:  # Separate pip cache for Unified Inference (SGLang, PyTorch, SentenceTransformers)
  pip-cache-backend:  # Separate pip cache for Backend (FastAPI, LangChain, etc.)
  npm-cache:          # Persistent npm download cache for frontend

